<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[漫画下载(十五)]]></title>
    <url>%2F2020%2F09%2F17%2F%E6%BC%AB%E7%94%BB%E4%B8%8B%E8%BD%BD%EF%BC%88%E5%A4%84%E7%90%86%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%E5%8F%8A%E7%AE%80%E5%8D%95%E5%8F%8D%E7%88%AC%E8%99%AB%EF%BC%89(%E5%8D%81%E4%BA%94)%2F</url>
    <content type="text"><![CDATA[在动漫之家选择一本漫画下载，下载一本已完结的。《一条狗》url=https://www.dmzj.com/info/yitiaogou.html 想下载这本动漫，需要保存所有章节的图片到本地。先捋捋思路： 拿到所有章节名和章节链接 根据章节链接爬取章节里的所有漫画图片 根据章节名，分类保存漫画 ** 获取章节名和章节链接 分析一下html 分析可以发现div标签下有个ul标签，ul标签是距离a标签最近的标签。 用上一篇文章讲解的Beautiful Soup，实际上直接匹配最近的class属性为list_con_li的ul标签即可。代码如下： 123456789101112131415161718import requestsfrom bs4 import BeautifulSouptarget_url=&apos;https://www.dmzj.com/info/yitiaogou.html&apos;req=requests.get(url=target_url)html=req.textbs=BeautifulSoup(html,&apos;lxml&apos;)list_con_li=bs.find(&apos;ul&apos;,class_=&apos;list_con_li&apos;)comic_list=list_con_li.find_all(&apos;a&apos;)chapter_name=[]chapter_urls=[]for comic in comic_list: href=comic.get(&apos;href&apos;) name=comic.text chapter_name.insert(0,name) chapter_urls.insert(0,href)print(chapter_name)print(chapter_urls) 章节名和章节链接就搞定了。 获取漫画图片地址 我们只要分析在一个章节里怎么获取图片，就能批量的在各个章节获取漫画图片。 我们先看第一章的内容。 url：https://www.dmzj.com/view/yitiaogou/34449.html#@page=1 打开第一章的链接，你会发现，链接后面自动添加了#@page=1但是这个并不是图片的真实地址，而是这个页面的地址，要下载图片，我们首先要拿到真实地址，审查元素找图片，页面无法右键这就是反爬虫手段，不过我们可以通过F12调出审查元素窗口。有的网站甚至还会把F12都禁掉，这也是反爬虫手段。面对这种禁止看页面源码的初级手段，一个优雅的通用解决方法就是，在链接前面加个view-source:。 view-source:https://www.dmzj.com/view/yitiaogou/34449.html 用这个链接，直接看的就是页面源码。 更简单的办法是，将鼠标焦点放在浏览器地址栏，然后按下F12调出调试窗口。在Network中找到我们的图片真实地址，链接如下： 1https://images.dmzj.com/img/chapterpic/175/866/14417836509461.jpg 这就是图片的真实地址，拿着这个链接去html页面中搜索，可以找到是有这个图片链接的。 但是用view-source:https://www.dmzj.com/view/yitiaogou/34449.html打开的页面找不到这个图片链接。说明这个图片是**动态加载**的。 view-source:方法只能看页面源码，不管动态加载的内容，这里面没有图片链接，就说明图片是动态加载的。使用JavaScript动态加载，无外乎两种方式： 外部加载 内部加载 外部加载就是在html页面中，以引用的形式，加载一个js，例如这样： 这段代码的意思是，引用xxxxxx.com域名下的call.js文件。内部加载就是javascript脚本内容写在html内，例如这个漫画网站。一般这种动态加载都是程序合成的。12345&lt;script type=&quot;text/javascript&quot;&gt; var arr_img = new Array(); var page = &apos;&apos;; eval(function(p,a,c,k,e,d)&#123;e=function(c)&#123;return c.toString(36)&#125;;if(!&apos;&apos;.replace(/^/,String))&#123;while(c--)&#123;d[c.toString(a)]=k[c]||c.toString(a)&#125;k=[function(e)&#123;return d[e]&#125;];e=function()&#123;return&apos;\\w+&apos;&#125;;c=1&#125;;while(c--)&#123;if(k[c])&#123;p=p.replace(new RegExp(&apos;\\b&apos;+e(c)+&apos;\\b&apos;,&apos;g&apos;),k[c])&#125;&#125;return p&#125;(&apos;7 8=\&apos;&#123;&quot;9&quot;:&quot;6&quot;,&quot;5&quot;:&quot;0&quot;,&quot;2&quot;:&quot;3\\/4\\/a\\/b\\/h.i&quot;,&quot;g&quot;:&quot;1&quot;,&quot;d&quot;:&quot;e&quot;,&quot;j&quot;:&quot;\\c\\f&quot;&#125;\&apos;;&apos;,20,20,&apos;||page_url|img|chapterpic|hidden|34449|var|pages|id|175|866|u7b2c01|chapter_order|10|u8bdd|sum_pages|14417836509461|jpg|chapter_name&apos;.split(&apos;|&apos;),0,&#123;&#125;))&lt;/script&gt; 图片链接：https://images.dmzj.com/img/chapterpic/175/866/14417836509461.jpg 由图可以看出链接的这几个数字就是合成的，可以把这些数字弄出来，拼接成图片链接。 未完。。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬取小说（十四）]]></title>
    <url>%2F2020%2F09%2F05%2F%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1.背景介绍：小说网站，“新笔趣阁”：https://www.xsbiquge.com/“新笔趣阁”只支持在线浏览，不支持小说打包下载。本文就是练习下载一篇名为《奇门地师》的网络小说。2.爬虫步骤爬虫其实很简单，可以大致分为三个步骤： 发起请求：我们需要先明确如何发起HTTP请求，获取到数据。 解析请求：获取到的数据乱七八糟，我们需要提取出我们想要的数据。 保存数据：将我们想要的数据，保存下来。 发起请求，就用request就行。解析的工具有很多，比如xpath、Beautiful Soup、正则表达式等。本文就用一个简单的经典小工具，Beautiful Soup来解析数据。保存数据，就是常规的文本保存。 3.Beautiful Soup安装我们可以使用pip来安装，在cmd命令窗口中输入如下命令安装：1pip install beautifulsoup4 安装好后，还需要安装lxml，这是解析HTML需要用到的依赖：1pip install lxml 4.开始我们先看下《奇门地师》小说的第一章内容。https://www.xsbiquge.com/97_97912/441675.html 我们先获取HTML信息试一试，编写代码如下：1234567import requestsif __name__==&apos;__main__&apos;: target=&apos;https://www.xsbiquge.com/97_97912/441675.html&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; print(req.text) 爬虫的第一步“发起请求”，得到的结果如下： 可以看到，我们获取了html信息，里面有我们想要的小说正文内容，但是也包含了一些其他内容，我们不关心div、br这些html标签 如何把正文内容从这些众多的html标签中提取出来? 进入爬虫的第二步“解析数据”也就是使用Beautiful Soup进行解析 现在，对目标页面进入审查元素，会看到： 文章的内容存在了id=content的div标签里，可以使用Beautiful Soup提取我们想要的正文内容，代码如下：12345678910import requestsfrom bs4 import BeautifulSoupif __name__==&apos;__main__&apos;: target=&apos;https://www.xsbiquge.com/97_97912/441675.html&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) texts=bs.find(&apos;div&apos;,id=&apos;content&apos;) print(texts) bs.find(‘div’,id=’content’)的意思就是，找到id属性为content的div标签。 可以看到，正文内容已经顺利提取，但是里面还有一些div和br这类标签，我们需要进一步清洗数据。 1234567891011import requestsfrom bs4 import BeautifulSoupif __name__==&apos;__main__&apos;: target=&apos;https://www.xsbiquge.com/97_97912/441675.html&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) texts=bs.find(&apos;div&apos;,id=&apos;content&apos;) print(texts.text.strip().split(&apos;\xa0&apos;*4)) texts.text是提取所有文字，然后在使用strip方法去掉回车，最后使用split方式根据\xa0切分数据，因为每一段的开头，都有四个空格。 扩展： \xa0 是不间断空白符&nbsp; 我们通常所用的空格是\x20，是在标准ASCII可见字符，0x20~0x7e范围内。而\xa0属于latin1(ISO/IEC_8859_1)中的扩展字符集，代表空白符&nbsp;(non-breaking_apsce).latin1字符集向下兼容ASCII（0x20~0x7e）。通常我们见到的字符多数是latin1的。 \u3000 是全角的空白符 根据Unicode编码标准及其基本多语言的定义，\u3000属于CJK标点符号区块内，是空白符之一。它的名字是 Ideographic Space ，有人译作表意字空格、象形字空格等。顾名思义，就是全角的 CJK 空格。它跟 nbsp 不一样，是可以被换行间断的。常用于制造缩进。 程序运行结果如下： 所有的内容，已经清洗干净，保存到一个列表里了。小说正文，已经顺利获取到了。要想下载整本小说，我们就要获取每个章节的连接，我们先分析下小说目录：https://www.xsbiquge.com/97_97912 审查元素后发现，所有章节信息，都存放到了id属性为list的div标签下的a标签内，代码： 123456789101112import requestsfrom bs4 import BeautifulSoupif __name__==&apos;__main__&apos;: target=&apos;https://www.xsbiquge.com/97_97912/&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) chapters=bs.find(&apos;div&apos;,id=&apos;list&apos;) chapters=chapters.find_all(&apos;a&apos;) for chapter in chapters: print(chapter) bs.find(‘div’,id=’list’)就是找到id属性为list的div标签，chapters.find_all(‘a’)就是在找到的div标签里，在提取出所有a标签，运行结果： 可以看到章节链接和章节名我们已经提取出来，但是还需要进一步解析，代码如下：123456789101112131415import requestsfrom bs4 import BeautifulSoupif __name__==&apos;__main__&apos;: server=&apos;https://www.xsbiquge.com&apos; target=&apos;https://www.xsbiquge.com/97_97912/&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) chapters=bs.find(&apos;div&apos;,id=&apos;list&apos;) chapters=chapters.find_all(&apos;a&apos;) for chapter in chapters: url=chapter.get(&apos;href&apos;) print(chapter.string) print(server+url) chapters.get(‘href’)方法提取了href属性，并拼接出属性url，使用chapters.string方法提取了章节名。 每个章节的链接，章节名，章节内容都有了，接下来就是整合代码，将内容保存到txt即可。 12345678910111213141516171819202122232425262728293031import requestsfrom bs4 import BeautifulSoupfrom tqdm import tqdmdef get_content(target): req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) texts=bs.find(&apos;div&apos;,id=&apos;content&apos;) content=texts.text.strip().split(&apos;\xa0&apos;*4) return contentif __name__==&apos;__main__&apos;: server=&apos;https://www.xsbiquge.com&apos; book_name=&apos;奇门地师.txt&apos; target=&apos;https://www.xsbiquge.com/97_97912/&apos; req=requests.get(url=target) req.encoding=&apos;utf-8&apos; html=req.text bs=BeautifulSoup(html,&apos;lxml&apos;) chapters=bs.find(&apos;div&apos;,id=&apos;list&apos;) chapters=chapters.find_all(&apos;a&apos;) for chapter in tqdm(chapters): url=server+chapter.get(&apos;href&apos;) chapters_name=chapter.string content=get_content(url) with open(book_name,&apos;a&apos;,encoding=&apos;utf-8&apos;) as f: f.write(chapters_name) f.write(&apos;\n&apos;) f.write(&apos;\n&apos;.join(content)) f.write(&apos;\n&apos;) 这样找到保存的txt文件，就可以看到下载的所有内容了。 下载过程中，如果我们使用了tqdm显示下载进度，让下载更加“优雅”，如果没有安装tqdm，可以使用pip进行安装 pip install tqdm 可以看到，小说内容保存到“奇门地师.txt”中，小说一共620章，下载大约七分钟，这样爬取数据很慢，可以使用分布式爬取。使用分布式可以一秒下完。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[requests模块使用（四）]]></title>
    <url>%2F2020%2F08%2F31%2Frequests%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[爬虫的原理就是写代码自动化的获取数据，保存下来数据，那怎么写代码来请求一个网址，获取结果，就要用到requests模块了。requests模块是python的一个第三方的模块，它是基于python自带的urllib模块封装的，用来发送http请求和获取返回的结果。安装requests1pip install requests requests模块用法123456789import requestsresponse=requests.get(&quot;http://www.baidu.com&quot;)print(type(response))print(response.status_code)print(type(response.text))print(response.text)print(response.cookies)print(response.content)print(response.content.decode(&quot;utf-8&quot;)) 我们可以看出response使用起来确实非常方便，这里有个问题需要注意一下：很多情况下的网站如果最直接response.text会出现乱码问题，所以这个使用response.content这样返回的数据格式其实是二进制格式，然后通过decode()转换为utf-8，这样就解决了通过response.text直接返回显示乱码的问题。 请求发出后，requests会基于HTTP头部对响应的编码作出有根据的推测。当你访问response.text之时，requests会使用其推测的文本编码。你可以找出Requests使用了什么编码，并且能够使用response.encoding属性来改变它。如：1234import requestsresponse=requests.get(&quot;http://www.baidu.com&quot;)response.encoding=&quot;utf-8&quot;print(response.text) 不管是通过response.content.decode(“utf-8”)的方式还是通过response.encoding=”utf-8”的方式都可以避免乱码问题的发生。 各种请求方式requests里提供各种请求方式123456import requestsrequests.post(&quot;http://httpbin.org/post&quot;)requests.put(&quot;http://httpbin.org/post&quot;)requests.delete(&quot;http://httpbin.org/post&quot;)requests.head(&quot;http://httpbin.org/post&quot;)requests.options(&quot;http://httpbin.org/post&quot;) 请求基本GET请求123import requestsresponse=requests.get(&quot;http://httpbin.org/get&quot;)print(response.text) 带参数的GET请求，例子1123import requestsresponse=requests.get(&quot;http://httpbin.org/get?name=Marry&amp;age=23&quot;)print(response.text) 如果我们想要在URL查询字符串传递数据，通常我们会通过http://httpbin.org/get?key=value方式传递。requests模块允许使用params关键字传递参数，以一个字典来传递这些参数，例子如下：12345678import requestsdata=&#123;&quot;name&quot;:&quot;Marry&quot;,&quot;age&quot;:23&#125;response=requests.get(&quot;http://httpbin.org/get&quot;,params=data)print(response.url)print(response.text) 上述两种的结果是相同的，通过params参数传递一个字典内容，从而直接构造url注意：第二种方式通过字典的方式的时候，如果字典中的参数为None则不会添加到url上 解析JSON1234567import requestsimportjsonresponse=requests.get(&quot;http://httpbin.org/get&quot;)print(type(response.text)) ----&gt;&lt;class &apos;str&apos;&gt;print(response.json())print(json.loads(response.text))print(type(response.json()))----&gt;&lt;class &apos;dict&apos;&gt; 从结果可以看出request里面集成的json其实就是执行了json.loads()方法，两者的结果是一样的 获取二进制数据在上面提到了response.content，这样获取的数据是二进制数据，同样的这个方法也可用于下载图片以及视频资源 添加headers和前面我们将urllib模块的时候一样，我们同样可以定制headers的信息，如当我们直接通过requests请求知乎网站的时候，默认是无法访问的123import requestsresponse=requests.get(&quot;https://www.zhihu.com&quot;)print(response.text) 这样会得到如下错误：因为访问知乎需要头部信息，这个时候我们在谷歌浏览器里输入chrome://version/ 就可以看到用户代理，将用户代理添加到头部信息。123456import requestsheaders=&#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0(WindowsNT6.1;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/85.0.4183.83Safari/537.36&apos;&#125;response=requests.get(&quot;https://www.zhihu.com&quot;,headers=headers)print(response.text) 这样就可以正常访问知乎了。 基本的POST请求通过在发送POST请求时添加一个data参数，这个data参数可以通过字典构造成，这样对于发送post请求就非常方便。1234567import requestsdata=&#123;&quot;name&quot;:&quot;Marry&quot;,&quot;age&quot;:23&#125;response=requests.post(&quot;http://httpbin.org/post&quot;,data=data)print(response.text) 同样在发送POST请求的时候也可以和发送get请求一样通过headers参数传递一个字典类型的数据。 响应我们可以通过response获得很多属性，例子如下1234567import requestsresponse=requests.get(&quot;http://www.baidu.com&quot;)print(type(response.status_code),response.status_code)print(type(response.headers),response.headers)print(type(response.cookies),response.cookies)print(type(response.url),response.url)print(type(response.history),response.history) 结果如下:状态码判断1234import requestsresponse=requests.get(&quot;http://www.baidu.com&quot;)ifresponse.status_code==requests.codes.ok:print(&quot;访问成功&quot;) requests高级用法文件上传实现方法和其他参数类似，也是构造一个字典后通过files参数传递1234import requestsfiles=&#123;&quot;files&quot;:open(&quot;m.jpg&quot;,&quot;rb&quot;)&#125;response=requests.post(&quot;http://httpbin.org/post&quot;,files=files)print(response.text) 获取cookie12345import requestsresponse=requests.get(&quot;http://www.baidu.com&quot;)print(response.cookies)forkey,valueinresponse.cookies.items():print(key+&quot;=&quot;+value) 会话维持Cookie的一个作用就是可用于模拟登陆，做会话维持12345import requestss=requests.Session()s.get(&quot;http://httpbin.org/cookies/set/number/123456&quot;)response=s.get(&quot;http://httpbin.org/cookies&quot;)print(response.text)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求（三）]]></title>
    <url>%2F2020%2F08%2F30%2FHTTP%E8%AF%B7%E6%B1%82%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[爬虫就是发送http请求（浏览器里面打开发送的都是http请求），然后获取到response，咱们再从response里面找到想要的数据，存储到本地。接下来就说一下什么是http请求，它里面都有哪些东西，我们在写爬虫的时候，怎么http请求，里面哪些对我们的爬虫有影响。 http请求过程咱们打开一个网站的时候，过程是这样的客户端（浏览器）发送请求到服务端（你打开的网站所在的服务器），服务端接收到请求，处理，返回数据给客户端（浏览器），然后在浏览器里面看到了数据。 请求方式主要有：GET/POST两种类型常用，另外还有HEAD/PUT/DELETE/OPTIONSGET和POST的区别就是：请求的数据GET是在url中，POST则是存放在请求体里面。 GET:一般向服务器获取数据用get请求，get请求的数据都是放在url中的，实质上和post请求没有太大的区别，当然也可以用来向服务器发送数据。 POST:一般向服务器发送数据用post请求，post请求的数据放在请求体里。 HEAD：与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。 PUT：向指定资源位置上传其最新内容。 OPTIONS：这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’*’来代替资源名称，向Web服务器发送OPTIONS请求，可以测试服务器功能是否正常运作。 DELETE：请求服务器删除Request-URI所标识的资源。 请求urlURL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 URL的格式由三个部分组成： 第一部分是协议(或称为服务方式)。 http/https 第二部分是存有该资源的主机IP地址(有时也包括端口号)。 第三部分是主机资源的具体地址，如目录和文件名等。 /index 爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据。 请求头一个请求由两部分组成， 请求头和请求体。 包含请求时的头部信息，如User-Agent,Host,Cookies等信息，user-agent就是你请求用的是什么浏览器，host就是服务端的地址，还有很多信息，服务端是如何分辨你是用的什么浏览器，你的ip地址就是从请求头里面获取到的。 请求体请求体就是发送数据的时候，数据放在请求体里面。get请求是没有请求体的，post请求才有请求体。 http响应发送了请求，服务端要给返回数据。这个就是响应，请求是你发出去的，响应是服务端返回给你的。 响应包含了2个部分，一个是响应头，一个是响应体。响应头里面包含了响应的状态码，返回数据的类型，类型的长度，服务器信息，Cookie信息等等。 响应体里面就是具体返回的数据了。 响应状态码有很多响应状态，不同的状态码代表不同的状态，常见的状态码如：200代表成功，301跳转，404找不到页面，502服务端错误 1xx消息——请求已被服务器接收，继续处理 2xx成功——请求已成功被服务器接收、理解、并接受 3xx重定向——需要后续操作才能完成这一请求 4xx请求错误——请求含有词法错误或者无法被执行 5xx服务器错误——服务器在处理某个正确请求时发生错误 常见代码： 200 OK 请求成功 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 301 目标暂时性转移 302 目标永久性转移]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬虫介绍（二）]]></title>
    <url>%2F2020%2F08%2F29%2F%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、什么是爬虫?爬虫：爬虫就是请求网站并提取数据的自动化程序。其中请求、提取、自动化是爬虫的关键。百科：网络爬虫（又称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。 二、爬虫可以干什么?爬虫可以帮你爬到你想要的东西，比如说你想要下载某个网站上面的图片、小视频、文章、文件，或者说你们公司想获取到对手公司网站上的一些数据用来分析市场，或者想要获取某一类网站用户的行为，用来分析用户以后的走向，都可以用爬虫来获取到数据。再比如说你想要做个什么内容类的app，类似今日头条的，那它里面的这些内容从哪里来的，它就是用爬虫、爬各种网站上热点新闻、八卦信息等等，再经过自己的加工放给用户去看。 三、爬虫的原理和实质:要从一个网站下载一个图片的话怎么办，要浏览器里面打开这个网站，然后右键保存图片到本地。那爬虫呢，就是写代码把上面的这个过程自动化，自动做这个操作，不需要在手动点了。这就是爬虫的原理。那爬虫的实质呢，就是写代码发http请求（浏览器里面打开发送的都是http请求），然后获取到response，咱们再从response里面找到想要的数据，存储到本地。 四、爬虫协议是什么？ 爬虫协议就是你想用爬虫爬我的网站，那么你得听我的，哪些你能爬，哪些你不能爬。 爬虫协议也称作Robots协议、机器人协议等，它的全称是“网络爬虫排除标准”（Robots Exclusion Protocol） 怎么查看一个网站的爬虫协议呢，就是在这个网站的域名后面加上robots.txt 比如说下面有：jd、百度、淘宝的爬虫协议 jd：https://www.jd.com/robots.txt 淘宝的：https://www.taobao.com/robots.txt 百度的：https://www.baidu.com/robots.txt 如果你要爬的网站域名加上robots.txt是404，那你就可以随心所欲的爬了。但是我们还是应该做一个“遵纪守法”的好爬虫。 爬虫协议里面有这么几个字段： User-agent：这个字段的意思是允许哪个引擎的爬虫获取数据 代表所有类型的爬虫都可以Disallow:/admin/这个字段代表爬虫不允许爬哪个路径下面的数据，如果是/的话，就代表所有的路径下面的数据都不能爬。 五、什么是反爬虫因为会有一些恶意的人，恶意的去用爬虫爬咱们的系统，获取一些数据用来做一些不好的事情，这样就会对我们的网站造成伤害。 那么反爬虫就是干这个事的，网站后台有程序专门检测发这个请求是爬虫发的，还是用户的正常请求（发请求就是打开一个页面），如果是爬虫发的话，那么就不给它返回数据，这就是返爬虫。当然有反爬虫那就有反爬虫的策略，就是看谁技术高低的问题了。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Windows安装Scrapy（一）]]></title>
    <url>%2F2020%2F08%2F28%2FWindows%E5%AE%89%E8%A3%85Scrapy%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Scrapy是python开发的一个爬虫框架;Scrapy很多模块都是基于Linux下的，所以在Windows上面安装的时候，可能会有各种各样的问题。 1.直接安装pip install scrapy 2.安装的时候scrapy它要依赖很多模块，一般都是其他的模块会报错。 3.在pip安装scrapy的时候，他会自动安装其他依赖的模块，安装到哪个模块报错了，它会停止安装。 4.查看报错模块缺少的是哪个依赖模块在进行安装即可。根据自己的情况来。 如：我安装的时候依赖缺少twisted库，即安装twisted库再安装scrapy twisted库不能通过pip 进行安装，可通过whl文件进行安装。进入：https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted cp后面代表python版本，win后为计算机位数。需依据自己的情况选择合适的下载。 123456789101112131415**Twisted: an event-driven networking engine.**Twisted-20.3.0-cp39-cp39-win amd64.whl Twisted-20.3.0-cp39-cp39-win32.whl Twisted-20.3.0-cp38-cp38-win amd64.whl Twisted-20.3.0-cp38-cp38-win32.whl Twisted-20.3.0-cp37-cp37m-win amd64.whl Twisted-20.3.0-cp37-cp37m-win32.whl Twisted-20.3.0-cp36-cp36m-win amd64.whl Twisted-20.3.0-cp36-cp36m-win32.whl Twisted-19.10.O-cp35-cp35m-win amd64.whl Twisted-19.10.O-cp35-cp35m-win32.whl Twisted-19.10.O-cp27-cp27m-win amd64.whl Twisted-19.10.o-cp27-cp27m-win32.whl Twisted-18.9.0-cp34zcp34m-win amd64.whl Twisted-18.9.0-cp34-cp34m-win32.whl 找到twisted库，下载到本地。下载好后，进入twisted刚刚下载所在的目录进行安装 我的电脑是win64位，python3.8的就下载这个Twisted‑20.3.0‑cp38‑cp38‑win_amd64.whl 安装：1pip install Twisted‑20.3.0‑cp38‑cp38‑win_amd64.whl]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-断言函数]]></title>
    <url>%2F2020%2F08%2F15%2FRobot-Framework-%E6%96%AD%E8%A8%80%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[测试用例的目的是要验证一些操作否符合我们的预期结果，所以在测试用例中，断言函数是必不可少的一项。我们做的每一步操作都会有预期的结果，为了保证操作得到的结果符合预期，我们需要在测试用例中添加断言，来保证实际结果和预期结果一致。常用的断言函数： 1、should be equal 与should not be equal第一行设置一个变量并赋值为1，第二行变量${var}和1应该是相等的。运行：会发现只是打印出了变量的值，一般来说，断言函数只起断言作用，符合断言没有任何操作，不符合则会报错：现在我们把${var}值改为2，断言不变，再运行：得到结果2！=1，用例报红，并且这里给出了断言出错。 should not be equal恰好相反，用来断言不相等。 2、should be empty与should not be empty断言为空或不为空。如上图，Create List是一个创建列表的函数，我们没有为列表赋值，则${var}是一个空列表，运行：可以看到打出了预期的空列表，并且用例成功运行。should not be empty恰好相反，用来断言不为空。 3、should contain、should not contain与should contain x times这里先说明一下，列表变量也可以用@{var}表示，但${var}既可以表示单个变量，也可以表示列表，字典，用起来方便，我们创建了一个列表，内含a，b，c三个值，断言列表中含有a：运行通过，打印出了变量值，可以看到正如我们的预期，${var}是一个列表。should not contain恰好相反，断言为不包含。should contain x times根据英文翻译即可，就是应该含有某值x次：这里的断言意思是变量${var}中应该包含2个1，运行： 4、should be equal as numbers与should not be equal as numbers以number的形式来进行比较，示例：should not be equal as numbers相反，不应等于数字。 5、should be equal as integers与should not be equal as integers以整数的形式来进行比较，示例：should not be equal as integers相反，不应等于整数。 6、should be equal as strings与should not be equal as strings以字符串的形式来进行比较，示例：should not be equal as strings相反，不应与字符串相等。 7、should start with与should not start with判断某个字符串是否以预期执行的字符串开始，如果以指定的字符串开头，则执行成功，否则执行失败，示例：与Should Start With刚好相反，如果以指定的字符串开头，则执行失败，否则执行成功，示例： 8、should end with与should not end with判断某个字符串是否以预期执行的字符串结尾，如果以指定的字符串结尾，则执行成功，否则执行失败，示例：与Should End With刚好相反，如果以指定的字符串结尾，则执行失败，否则执行成功。 9、should match与should not match 判断某个字符串是否与预期指定的字符串相匹配，如果可以匹配，则执行成功，否则执行失败，示例：与Should Match刚好相反，如果字符串匹配，则执行失败，否则执行成功，示例：]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-常用快捷键]]></title>
    <url>%2F2020%2F08%2F15%2FRobot-Framework-%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[robot framework常用快捷键，记录下来，下次使用不记得就不用百度了,需要注意一点，如果快捷键不能使用，先看看是否有其他软件已占用相应的快捷键。 重命名——》F2 搜索关键字——》F5 执行用例——》F8 创建新工程——》ctrl+n 创建新测试套——》ctrl+shift+f 创建新用例——》ctrl+shift+t 创建新关键字——》ctrl+shift+k 向上移动用例——》ctrl+↑ 向下移动用例——》ctrl+↓ 显示关键字信息——》 ctrl+鼠标悬浮（鼠标悬浮于关键字上） 自动补全关键字——》ctrl+shift+空格 删除行——》ctrl+d 删除单元格——》ctrl+shift+d 插入单元格——》ctrl+shift+i 插入行——》ctrl+i 屏蔽代码——》ctrl+# 取消屏蔽——》ctrl+$ 保存整个工程——》ctrl+shit+s 局部保存，保存鼠标点击的部分——》ctrl+s 查看log——》ctrl+L 查看report——》ctrl+r]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-下拉列表]]></title>
    <url>%2F2020%2F08%2F14%2FRobot-Framework-%E4%B8%8B%E6%8B%89%E9%80%89%E6%8B%A9%E6%A1%86%2F</url>
    <content type="text"><![CDATA[下拉选择框很常见，选择下拉框有几种方式处理，首先在浏览器F12选择下拉框,F12后看见下拉框的源码是 option xxx，可使用Select From List By Index，Select From List By Label和Select From List By Label关键字1.Select From List By IndexArguments:[ locator | *indexes ]两个参数：一个是select元素的定位路径，一个是下拉选项的下标，从0开始，0表示选择第一个值。 2.Select From List By LabelArguments:[ locator | *lables]两个参数：一个是select元素的定位路径，一个是下拉选项的text值，注意是text值，不是标签的value值。 3.Select From List By ValueArguments:[ locator | *values]两个参数：一个是select元素的定位路径，一个是下拉选项的标签value值。 4.Click element直接定位到选择的元素 如果F12后看到的下拉源码是这样的： div xxxx ，使用下面方式 Click Element xpath = //xxx/div[2] #先点击下拉框显示出来 Click Element xpath=//xxx/xxx//div[text()=’用户A’] #然后再点击所要选择的下拉内容]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework--分层思想]]></title>
    <url>%2F2020%2F08%2F14%2FRobot-Framework-%E5%88%86%E5%B1%82%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[在Java中，程序设计讲究设计模式，设计模式其实就是根据需求使用抽象和封装，其实这个就是分层思想，把一个实现过程分成不同多层，提高灵活性，从而达到可扩展性和可维护性。要谈RF的分层思想，就不得不说到关键字驱动，刚刚我们介绍了关键字驱动：类似函数，通过调用不同的关键字，从而测试结果就不同。在RF中，我们可以把操作步骤封装一个一个的方法（关键字），通过调用关键字来实现测试用例。 比如我们要登录测试环境，创建下面的一条登录的测试用例。 现在我要写3条测试环境登录的用例： 可以在测试套件下创建3条测试用例，如果我们现在要检查登录，登陆不同的账号，其实只是输入的文本变了，而操作步骤和操作的元素都是不变的，如果这样做，无疑增加的脚本的冗余，而且不便于维护。假如，输入框的定位方式变了，我不得不打开每一条用例进行修改。 所以我们把一个个操作封装成关键字，放在一个专放操作的文件夹里，以便管理。需要用到是则去一一调用这些关键字，而不需要重复写这些步骤，从而实现分层的思想来解决这个问题。如下：右键“测试项目”选择“new resource”创建资源。输入资源名称：2、创建关键字右键“业务关键字”选择“new User Keyword” 来创建用户关键字。输入关键字的名称： 3、编辑关键字对于一个测试用例来说，用户关心的是输入什么内容，得到什么结果。所以，对于“登录”关键字来说，需要创建三个接口变量${mobile}，${password}和${result}三个变量，用于接收输入内容和预期结果。点击Arguments输入框，定义变量，多个变量从用“|”隔开。 在登录用户中使用参数化变量。 4、添加创建的资源切换到测试套件页面，添加资源（业务关键字.txt）5、调用关键字现在就可以在测试用例中使用创建的关键字了（登录页面）。对于每一条用例来说，调用“登录”关键字，输入手机号、密码，输入预期结果即可。不用关心用例是如何执行的。如果手机号码，密码输入框的定位发生了变化，只用去修改“登录”关键字即可，不用对每一条用例做任何修改。大大提高的用例的维护性和扩展性。 继续分层的设计： 为了以后维护更方便，可以再把元素定位分离出来，放在一个专放元素定位的文件夹里…如果以后页面发生了调整，我们就需要修改元素的定位，就不用一条条用例打开去修改，只要修改页面定位元素即可如下：]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-FAIL For loop has no closing 'END']]></title>
    <url>%2F2020%2F08%2F13%2FFAIL-For-loop-has-no-closing-END%2F</url>
    <content type="text"><![CDATA[今天练习条件与循环跟着网上的例子执行循环时，一直执行不通过，写法都一样，一直报错FAIL For loop has no closing ‘END’ 最后发现输入:FOR时冒号自动消失了，于是打开F5查看了:FOR关键字，发现文档写着已经不赞成使用了，使用请查看FOR关键字.RIDE最新版本和之前的版本在用例脚本编写还是有一些差异存在的，如果冒号（:）丢失就要在FOR循环语句后加上END。 RIDE1.7.4.1版本的FOR循环语句结构与RIDE之前版本稍微有些变化，需要在FOR循环语句结束后加上END（注意END一定要大写）。 加上END后就可以运行成功了，正确用例脚本如下：]]></content>
      <categories>
        <category>问题+解决方法记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-关键字]]></title>
    <url>%2F2020%2F08%2F13%2FRobot-Framework-Selenium2Library%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[关键字的使用可以通过F5查找关键字库，输入关键字，点击搜索。选择关键字就可以查看关键字的说明。如图： 1、浏览器驱动 通过不同的浏览器执行脚本 Open Browser https://www.baidu.com Chrome 常用浏览器对应的关键字 firefox Firefox chrome Chrome ie InternetExplorer safari Safari open browser 同样也可以打开本地html页面，如： Open Browser file:///D:/RFpath/js.html Chrome 备注：要想通过不同的浏览器打开url地址，一定要安装对应的驱动。chrome的驱动为：chromedriver.exefirefox的驱动为：geckodriver.exe…浏览器默认为空时启动FireFox 2、关闭浏览器 Close Browser Close All Browser Close Browser关闭当前的浏览器，Close All Browser 关闭所有打开的浏览器和缓存重置。 3、浏览器最大化 Maximize Browser Window 4、设置浏览器宽、高 Set Window Size 800 600 Set Window Size关键字，用于设置当前浏览器的宽度和高度，以像素为单位，第一个参数800表示宽度，第二个参数600表示高度。 Get Window Size关键字，用于获取当前浏览器的宽度和高度，获得浏览器的宽、高并打印，如下： 5、文本输入input text用于向文本框输入信息，id=KW表示元素定位，定位文本框输入 6、点击按钮 7、点击元素 8、等待页面元素出现 Wait Until Page Contains Element关键字用于等待页面上的元素显示出来。//*[@id=”container”]/div[2]/div/div[2]/span:表示元素定位，这里定位出现的元素60：表示最长等待时间error:表示错误提示，可以自己自定义错误提示 如：元素不能正常显示 9、获取title Get Title关键字用于获得当前浏览器窗口的title信息。Should Contain 比较${title}是否等于“robot_百度搜索”这里只获得title是没有意义的，我们通常会将获取的title传递给一个变量，然后与预期结果进行比较。从而判断当前脚本执行成功。 10、获取Text Get Text关键字用于获取元素的文本信息//*[@id=”container”]/div[2]/div/div[2]/span:表示定位文本信息的元素 11、获取元素属性值 id=kw 表示定位的元素name 表示获取这个元素的 name 属性值。 12、log 打印log关键字就是编程语言里的“print”一样，可以打印任何你想打印的内容。 13、定义变量通过”Set Variable”关键字来定义变量 14、连接对象“Catenate”关键字可以连接多个信息加上“SEPARATOR=”可以对多个连接的信息进行分割。 15、定义列表通过“Create List”关键字可以定义列表。每个字符串前面加 u，是为了统一编码问题，将字符串转为 Unicode 编码。如果通过“@{}”去定义列表的话，可以通过“log many”关键字进行打印 16、时间操作 Robot Framework 中提供了“get time”关键字用来获取当前时间。 17、设置休眠时间 “sleep”关键字用来设置休眠一定时间，sleep 关键字默认以“秒”为单位。 18、if语句 通过“run keyword if”关键字可以编写 if 分支语句。 首先定义一个变量 a 等于 59 。If 判断 a 大于等于 90 ，满足条件 log 输出 “优秀 ”；不满足上面的条件，接着 else if 判断 a 大于等于 70 ，满足条件 log 输出 “良好”；不满足上面的条件，接着 else if 判断 a 大于等于 60 ，满足条件 log 输出 “及格”；上面的条件都不满足，else log 输出“不及格”。注：注意 ELSE IF 和 ELSE 前面的三个点点点（…）。注意ELSE IF和ELSE要是大写。 19、for循环在 Robot Framework 中编写循环通过“for”。通过“：for”定义 for 循环；in range 用于指定循环的范围。例子1，这个例子为执行 10 次循环注意：in range 定义为 10，它的范围是 0~9例 2，遍历列表“create list” 关键字用来定义列表（a,b,c），“@{}”用来存放列表。通过“for”循环来来遍历@{abc}列表中的字符。20、 Evaluate生成随机数21、注释Robot Framework 中添加注释也非常简单。（1）“Comment”关键字用于设置脚本中的注释。（2）也可以像 Python 一样使用“#”号进行注释]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework -元素定位]]></title>
    <url>%2F2020%2F08%2F13%2FRobot-Framework-%E5%85%83%E7%B4%A0%E5%AE%9A%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[1.id和name定位id、name定位，这两个比较简单直接可以id=和name=，前提是这个id和name的值在当前页面上是唯一的。12345…&lt;input id=&quot;kw&quot; name=&quot;wd&quot; class=&quot;s_ipt&quot; value=&quot;&quot; maxlength=&quot;255&quot; autocomplete=&quot;off&quot;&gt;…&lt;input type=&quot;submit&quot; id=&quot;su&quot; value=&quot;百度一下&quot; class=&quot;bg s_btn&quot;&gt;… 根据上面的例子，百度输入框可以提取id或name进行定位。 id=kw name=wd在Robot framework中可以这样写：Input Text 用于文本输入的关键字，“robot framework学习”是要给输入框输入的内容.百度按钮没有name，所以这里可以用id： id=suClick Button 是按钮点击的关键字。 2.Xpath定位Xpath是XML文档中定位元素的一种语言，HTML可以看成一种XML文档，Xpath定位也是所有定位元素的方法中用的最多的。Xpath 中的绝对路径从 HTML 根节点开始算，相对路径从任意节点开始。通过开发者工具，我们可以拷贝到 Xpath 的绝对路径和相对路径代码：但是由于拷贝出来的代码缺乏灵活性，也不全然准确。大部分情况下，都需要自己定义 Xpath 语句，因此 Xpath 语法还是有必要学习。 2.1绝对路径定位以百度中的输入框和按钮为例，通过拷贝出来的 full Xpath：1/html/body/div[1]/div[1]/div[5]/div/div/form/span[1]/input 这是一个绝对路径，绝对路径是从根节点/html开始往下找，html下面的body下面的div下面的第5个div下面的….input标签。通过一级一级的锁定就找到了想要的元素。 2.2相对路径定位除了绝对路径，Xpath中更常用的方式是相对路径定位方法，以“//”开头相对路径可以从任意节点开始，一般我们会选取一个可以唯一定位到的元素开始写，增加查找的准确性。 2.2.1元素属性定位Xpath可以利用元素自身的属性进行定位：1Xpath=//*[@id=’kw’] //表示某个层级下，*表示某个标签名。@id=kw 表示这个元素有个id等于kw（@ 符号指定需要使用的属性）也可以制定标签名1Xpath=//input[@id=”kw”] 元素本身，可以利用的属性不止局限于id和name,还可以使用其他属性，如：12Xpath=//input[@type=”text”]Xpath=//input[@autocomplete=&quot;off&quot;] 但是这些元素必须在这个页面是唯一的，否则定位时会出错。 2.2.2找上级遇到某些元素无法精确定位的时候，可以查找其父级及其祖先节点，找到有确定的祖先节点后通过层级依次向下定位。元素的上级属性为：123&lt;form id=&quot;form1&quot; class=&quot;fm&quot; action=&quot;/s&quot; name=&quot;f1&quot;&gt;&lt;span class=&quot;bg s_ipt_wr&quot;&gt;&lt;input id=&quot;kw&quot; class=&quot;s_ipt&quot; type=&quot;text&quot; maxlength=&quot;100&quot; name=&quot;wd&quot; autocomplete=&quot;off&quot;&gt; 找爸爸：1Xpath=//span[@class=” bg s_ipt_wr”]/input 如果爸爸没有唯一的属性，可以找爷爷：1Xpath=//from[@id=’from1]/span/input 这样一级一级找上去，直到html,那么就是一个绝对路径了 2.2.3使用逻辑运算符如果元素的某个属性无法精确定位到这个元素，我们还可以用逻辑运算符 and 连接多个属性进行定位，以百度输入框为例。1、 使用and:1//*[@name=&apos;wd&apos; and @class=&apos;s_ipt&apos;] 查找 name 属性为 wd 并且 class 属性为 s_ipt 的任意元素 2、 使用or:1//*[@name=&apos;wd&apos; or @class=&apos;s_ipt&apos;] 查找 name 属性为 wd 或者 class 属性为 s_ipt 的任意元素，取其中之一满足即可 3、使用|，同时查找多个路径，取或：1//form[@id=&quot;form&quot;]//span | //form[@id=&quot;form&quot;]//input 选取 form 下所有的 span 和所有的 input。 3.CSS定位CSS(Cascading Style Sheets)是一种语言，它被用来描述HTML 和XML 文档的表现。CSS 使用选择器来为页面元素绑定属性。CSS 可以比较灵活选择控件的任意属性，一般情况下定位速度要比XPath 快，但对于初学者来说比较难以学习使用，下面我们就详细的介绍CSS 的语法与使用。CSS 选择器的常见语法:同样以百度输入框的代码，我们来看看CSS如何定位。123&lt;form id=&quot;form1&quot; class=&quot;fm&quot; action=&quot;/s&quot; name=&quot;f1&quot;&gt; &lt;span class=&quot;bg s_ipt_wr&quot;&gt; &lt;input id=&quot;kw&quot; class=&quot;s_ipt&quot; type=&quot;text&quot; maxlength=&quot;100&quot; name=&quot;wd&quot; autocomplete=&quot;off&quot;&gt; id定位：1css=#kw class 定位：1css=.s_ipt 其他属性定位：123css=[name=wd]css=[type=text]css=[autocomplete=off] 父子定位：12Css=span&gt;inputCss=from&gt;span&gt;input 根据标签名定位：1Css=input 代码实现：]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-[ ERROR ] Suite 'TestProject' contains no tests matching name]]></title>
    <url>%2F2020%2F08%2F12%2F%5B%20ERROR%20%5D%20Suite%20'TestProject'%20contains%20no%20tests%20matching%20name%2F</url>
    <content type="text"><![CDATA[编写用例完成，运行测试用例，运行完成报错：1[ ERROR ] Suite &apos;TestProject&apos; contains no tests matching name &apos;TestProject.TestSuites.baidu_search&apos; in suite &apos;TestProject.TestSuites&apos;. 如图： 原因：是由于在创建测试套件（test suite）时，格式选择为TXT文件格式导致 解决办法：将文件修改为robot格式即可。]]></content>
      <categories>
        <category>问题+解决方法记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework--第一个脚本]]></title>
    <url>%2F2020%2F08%2F12%2FRobot-Framework-%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[1.Robot Framework介绍Robot Framework的架构是一个通用的验收测试和验收测试驱动开发的自动化测试框架（ATDD）。它具有易于使用的表格来组织测试过程和测试数据。 它使用关键字驱动的测试方法。对于上面的例子来说，Open Browser、input text、click element、sleep、Get Title、Should Contain和close browser，都是“关键字”，这些关键字由robotframework-selenium2library类库所提供。当然我们也可以自定义关键字。Robot Framework的操作系统和应用独立框架。核心框架是使用Python和运行在Jython(JVM)和IronPython(.NET) 2.Robot Framework入门下面一步一步来创建我们的第一条用例 1.创建测试项目选择菜单栏file—-&gt;new Project, Name 输入项目名称,Type 选择Directory 2.创建测试套件右键点击“测试项目”选择new Suite选项，Name 输入项目名称，Type 选择File，Format选择ROBOT（如果选的是TXT格式，运行用例时会报错） 3.创建测试用例右键点击“测试套件”选择new Test Case,点击OK即可。 导入Selenium2Library库 因为RF框架编写基于web 的测试用例，所以，我们需要selenium 的库支持。所以，我们在使用的过程中需要加载Selenium2Library库 在“测试套件”的Edit标签页，点击“Library”按钮，弹出输入框，Name输入：Selenium2Library，点击OK完成。如果导入的库显示红色，表示导入的库不存在。如果是黑色则表示导入成功。注意区分大小写。 5.编写用例下面就可以写我们的用例了，可以通过F5快捷键来查询库提供的关键字。 如上图，自动化脚本从打开浏览器开始，我想打开一个浏览器，自然想到的是以“open”为关键字搜索，结果找到了“Open Browser”的关键字，点击这个关键字，显示它的用法和说明。下面开始实操创建百度搜索用例如下： “Open Browser”变蓝了，说明它是一个合法的关键字，后面有一个方框是红色的，表示这个参数不能缺省的。通过说明信息中，发现它需要一个url 地址是必填的，当然还需要指定browser （默认不填为 friefox） 6.运行测试用例勾选当前需要运行的测试用例，点击工具栏运行按钮，如果只运行单个用例的话，也可以切换到用例的Run标签页，点击“start”按钮。 运行信息： 运行信息显示会生成三个文件：Output.xml、Log.html、Report.html我们重点查看Log.html和Report.html ，Log.html更关注脚本的执行过程的记录，Report.html更关注脚本的执行结果的展示。]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-python3.8与ride不兼容问题]]></title>
    <url>%2F2020%2F08%2F12%2Fpython3.8%E4%B8%8Eride%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[robotframework环境搭建完成，运行ride报错 1、robotframework 环境（pip list）：python 3.8.2robotframework 3.2.1wxpython 4.0.7ride 1.7.4.2 2、运行ride报错 百度上找了一圈，有人说是robotframework-ride最新版本1.7.4.2 不兼容 python 3.8，后来把1.7.4.2版本的卸载了1pip uninstall robotframework 又重新指定版本安装1.7.4.1的试试看：1pip install robotframework-ride==1.7.4.1 终于装上了！ 然而又报了新的错误… 又继续百度说是系统语言的问题，在 robotide 中找到应用程序配置项Lib\site-packages\robotide\application\application.py，看到初始加载的语言环境是英文。 加上一个局部支持的语言试试 把修改application.py中 self._initial_locale = wx.Locale(wx.LANGUAGE_ENGLISH)改为：1self.locale = wx.Locale(wx.LANGUAGE_ENGLISH) 又出现了新的问题.. 这次报的是缩进的问题，源码缩进格式不正确，简单粗暴的复制下面没问题的源码缩进，终于终于运行能够打开了！]]></content>
      <categories>
        <category>问题+解决方法记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework-RIDE桌面快捷方式制作]]></title>
    <url>%2F2020%2F08%2F12%2FRobot%20Framework-RIDE%E6%A1%8C%E9%9D%A2%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[接上一篇笔记0.0 1.创建快捷方式 在桌面右键鼠标，弹出的菜单选择 新建-快捷方式，然后在请键入对象的位置输入这一行命令 D:\Python38\pythonw.exe -c “from robotide import main; main()” 注：根据自己安装python的路径，我的是在D盘的，还有双引号记得是英文格式哦，否则会启动不了哦 如图： 点击下一步，输入快捷方式的名称，根据自己需要随意命名就好。 点击完成后会在桌面生成图标 2.如果想要把图标换成机器人图标需要快捷方式上点击右键-属性，如图 点击“更改图标”，在浏览里找到目录D:\Python38\Lib\site-packages\robotide\widgets，里面有个robot.ico的图标 选它就可以了，效果如图 最后快捷方式就制作完成了，可以打开正常使用了。]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Robot Framework环境搭建]]></title>
    <url>%2F2020%2F08%2F11%2FRobot%20Framework%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.Python3.8安装下载地址：https://www.python.org/downloads/release/python-381/ python-3.8.1.amd64.msi(python2官方已宣布到2020年元旦起不再维护，所以为了以后方便在这里安装python3) 下载完成后，选择安装路径自行安装即可。安装完成后注意配置系统环境变量path:配置Python38安装路径和Python38/Scripts路径。 2.Robot framework的安装RF框架是基于python的，所以一定要有python环境 安装方式选择一种即可： （1）exe包安装下载地址：https://pypi.org/project/robotframework/3.2.1/#downloads robotframework-3.2.1.win-amd64.exe 直接双击下一步即可。 （2）pip命令安装1pip install robotframework 这样子会直接安装最新的版本，我们可以指定版本安装pip install robotframework==3.2.1 3.wxPython 的安装作用：Wxpython 是python 非常有名的一个GUI库，因为RIDE 是基于这个库开发的，所以这个必须安装。 （1）exe包安装下载地址：http://sourceforge.net/projects/wxpython/files/wxPython/ 直接双击下一步即可。 （2）pip命令安装1pip install wxPython==4.0.7 在线安装 wxPython，最好指定版本安装，否则会直接安装最新的版本 （这个版本不能太低…太低不支持python3.8，太高也会报错…所以找了个版本不高，又支持3.8的wxPython==4.0.7） 4.Robot framework-ride作用：RIDE就是一个图形界面的用于创建、组织、运行测试的软件。 安装方式选一种即可： （1）包安装下载地址：https://pypi.python.org/pypi/robotframework-ride robotframework-ride-1.7.4.2.tar.gz 下载完成后将其解压，然后进入到解压后的文件路径进行安装123cd D:\robotframework-ride-1.7.4.2python setup.py install （2）pip安装1pip install robotframework-ride=1.7.4.1（安装了最新的1.7.4.2一直报错，所以我这里装的是1.7.4.1） 5.Robot framework-selenium2library作用：RF-seleniumlibrary 可以看做RF版的selenium 库，selenium （webdriver）可以认为是一套基于web的规范（API），所以，RF 、appium 等测试工具都可以基于这套API进行页面的定位与操作。 （1）包安装下载地址：https://github.com/robotframework/Selenium2Library#readme Selenium2Library-master.zip 下载完成后将其解压，然后进入到解压后的文件路径进行安装 12cd D:\Selenium2Library-masterpython setup.py install （2）pip安装1pip install robotframework-selenium2library 6.放入驱动以上步骤都已安装完毕后，把需要用的驱动放到python3的目录下，用谷歌浏览器较多，所以我把谷歌驱动文件chromedriver.exe放入我的python3安装目录D:\Python38 7.查看pybot版本12cd D:\Python38\Scriptspybot --version 8.启动RIDE 1.通过文件启动（双击D:\Python38\Lib\site-packages\robotide下的init.py文件） 2.通过命令启动（运行-&gt;ride.py回车/确认，打开ride.py文件之后（以python方式打开 ） 12cd D:\Python38\Scripts\python ride.py 3.将D:\Python38\Scripts\ride.py创建快捷键 注：根据自己的python安装路径，我的是在D盘的]]></content>
      <categories>
        <category>Robot Framework</category>
      </categories>
  </entry>
</search>
